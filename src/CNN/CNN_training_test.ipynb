{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, sys, os, random, time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mounting google drive\n",
    "collaboratory = False\n",
    "\n",
    "if collaboratory:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')\n",
    "    # !ln -s \"/gdrive/Shareddrives/HSC_project/HSC_ML\" \"/content/HSC_ML\"\n",
    "    os.chdir(\"/gdrive/Shareddrives/HSC_project/HSC_ML\")\n",
    "else: \n",
    "    print('Running on local systems, if running on collaboratory please change above')\n",
    "\n",
    "print(os.listdir())\n",
    "# print(os.getcwd())\n",
    "#!unzip ./HSC_ML/Output/TBCells_crops.zip -d ./HSC_ML/Output/\n",
    "\n",
    "if collaboratory:\n",
    "  image_folder = \"./Output/\"\n",
    "else:\n",
    "  image_folder = \"../../Output/TBCells_crops/Train_data/\"\n",
    "trained_weights = image_folder[:-11]+\"Results/Latest_Weight_CNN/\"\n",
    "\n",
    "\n",
    "if not os.path.exists(trained_weights):\n",
    "        os.makedirs(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic deformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pip install git+https://github.com/gvtulder/elasticdeform\n",
    "\n",
    "import numpy, imageio, elasticdeform\n",
    "from matplotlib import image\n",
    "\n",
    "augmented_data_req = False\n",
    "\n",
    "if augmented_data_req:\n",
    "    augmented_images = os.path.abspath(os.path.join(image_folder, os.pardir)) + \"/augmented_images/\"\n",
    "\n",
    "    if not os.path.exists(augmented_images):\n",
    "            os.makedirs(augmented_images)\n",
    "            \n",
    "\n",
    "    for folder in os.listdir(image_folder):\n",
    "        current_folder = os.path.join(image_folder, folder)\n",
    "        augmented_folder = augmented_images + folder + \"/\"\n",
    "        \n",
    "        if not os.path.exists(augmented_folder):\n",
    "            os.makedirs(augmented_folder)\n",
    "        for image_name in os.listdir(current_folder):\n",
    "            current_image = os.path.join(current_folder,  image_name)\n",
    "            # apply deformation with a random 3 x 3 grid\n",
    "            my_image = np.array(image.imread(current_image))\n",
    "            images_deformed = elasticdeform.deform_random_grid(my_image, sigma=3, points=2)\n",
    "            imageio.imsave( augmented_folder + image_name.split(\".\")[0] + \"_aug.tif\", images_deformed)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy generated images\n",
    "\n",
    "copy_augmented_data = False\n",
    "if copy_augmented_data:\n",
    "    import shutil\n",
    "    for folder in os.listdir(image_folder):\n",
    "        current_folder = os.path.join(image_folder, folder)\n",
    "        augmented_folder = augmented_images + folder + \"/\"\n",
    "        for image_name in os.listdir(augmented_folder):\n",
    "            current_image = os.path.join(augmented_folder,  image_name)\n",
    "            shutil.copy(current_image, current_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spliting dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ColorJitter(brightness = 0.25, contrast=0.25),\n",
    "    #transforms.RandomRotation(90),\n",
    "    transforms.RandomRotation(degrees=(90, 90)),\n",
    "    transforms.RandomRotation(degrees=(270, 270)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([transforms.Resize((128, 128)),transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "# add path of images https://gist.github.com/andrewjong/6b02ff237533b3b2c554701fb53d5c4d\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "dataset = ImageFolderWithPaths(image_folder, transform=train_transform)\n",
    "print(dataset.class_to_idx)\n",
    "train_indices, val_indices = sklearn.model_selection.train_test_split(\n",
    "                    list(range(dataset.__len__())),          # list of indices of dataset\n",
    "                    stratify = dataset.targets,                 # for stratify sampling the data due to the unblanced dataset\n",
    "                    random_state = 42,\n",
    "                    test_size=.25                               #represent the proportion of the dataset to include in the train split.\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "# subsetting the dataset into train and test\n",
    "training_data = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_data = torch.utils.data.Subset(dataset, val_indices) \n",
    "\n",
    "print(f\"Length of dataset:\" )\n",
    "print(f\" {'Train dataset':<12}   \\u2502   {'Test data':>12} \")\n",
    "print(f\" {len(training_data):<12}    \\u2502   {len(test_data) :>12}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating and Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"B Cells\",\n",
    "    1: \"T Cells\",\n",
    "    #2: \"T8 Cells\",\n",
    "\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label, _ = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.permute(1,2,0), cmap=\"gray\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "# setting sampler weight\n",
    "y_train_indices = training_data.indices\n",
    "y_train = [dataset.targets[i] for i in y_train_indices]\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in y_train])\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True) \n",
    "\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size= batch_size,sampler = sampler, pin_memory=True, )\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "for X, y, _ in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, in_features=3, classes=2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_features, 32, kernel_size=3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(128, classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Cuda acceleration enabled!\" if torch.cuda.is_available() else 'Running on CPU')\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    # initializing with resnet weight\n",
    "    #model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "    model = MyCNN()\n",
    "\n",
    "    # # get the input feature in fully connected layer\n",
    "    # num_in_features = model.fc.in_features\n",
    "\n",
    "    # # Replace the final fully connected layer to suite the problem\n",
    "    # model.fc = nn.Sequential(nn.Linear(num_in_features, 512),\n",
    "    #                                 nn.ReLU(),\n",
    "    #                                 nn.Dropout(0.3),\n",
    "    #                                 nn.Linear(512, 128),\n",
    "    #                                 nn.Linear(128, 32),\n",
    "    #                                 nn.Linear(32, len(labels_map)))\n",
    "    \n",
    "    # using gpu\n",
    "    model = model.cuda() if torch.cuda.is_available() else model\n",
    "    # print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation(model, train_dataloader, loss_fn):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for image, target, _ in train_dataloader:\n",
    "        image, target = image.to(DEVICE), target.to(DEVICE)\n",
    "        output = model(image)\n",
    "        loss = loss_fn(output, target)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    return np.array(losses).mean()\n",
    "\n",
    "\n",
    "def loss_fn(y_pred, y_true):\n",
    "    bce_fn = nn.CrossEntropyLoss()\n",
    "    loss = bce_fn(y_pred, y_true)\n",
    "    return loss\n",
    "\n",
    "\n",
    "### Table for results\n",
    "header = r'''\n",
    "        Train | Valid\n",
    "Epoch |  Loss |  Loss | Time, m | Save status\n",
    "'''\n",
    "#          Epoch         metrics            time\n",
    "raw_line = '{:6d}' + '\\u2502{:7.3f}'*2 + '\\u2502{:6.2f}' + '\\t' + '\\u2502 {}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# # not optimizing the whole network for now\n",
    "# # optimizer = torch.optim.AdamW(model.parameters(),\n",
    "# #                   lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "# ignored_params = list(map(id, model.fc.parameters()))\n",
    "# base_params = filter(lambda p: id(p) not in ignored_params,\n",
    "#                      model.parameters())\n",
    "\n",
    "learning_rate = 4e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 350\n",
    "vloss_min = np.Inf\n",
    "print(header)\n",
    "batch_losses = []\n",
    "vlosses = []\n",
    "\n",
    "for epoch in range(1, EPOCHES+1):\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for image, target, _ in train_dataloader:\n",
    "        image, target = image.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    vloss = validation(model, test_dataloader, loss_fn)\n",
    "    \n",
    "    batch_loss = np.array(losses).mean()\n",
    "    batch_losses.append(batch_loss)\n",
    "    \n",
    "    vlosses.append(vloss)\n",
    "    if vloss_min > vloss:\n",
    "        vloss_min = vloss\n",
    "        torch.save(model.state_dict(), trained_weights +'TBCells.pt')\n",
    "        save_status = 'Yes'\n",
    "    else:\n",
    "        save_status = 'No'\n",
    "        \n",
    "    print(raw_line.format(epoch, batch_loss, vloss,\n",
    "                              (time.time()-start_time)/60**1, save_status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting validation vs train error\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (9, 5))\n",
    "ax.legend(loc='best')\n",
    "plt.title(\"Train-Validation Accuracy\")\n",
    "plt.plot(batch_loss, label='train')\n",
    "plt.plot(vloss, label='validation')\n",
    "plt.xlabel('num_epochs', fontsize=12)\n",
    "plt.ylabel('accuracy', fontsize=12)\n",
    "fig.tight_layout()\n",
    "plt.savefig(trained_weights + \"Train_Validation_Accuracy.png\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct model from saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restart = False\n",
    "if restart:\n",
    "    model1 = torchvision.models.resnet50()\n",
    "    model1.fc = torch.nn.Sequential(\n",
    "        torch.nn.Linear(\n",
    "            in_features=2048,\n",
    "            out_features=1\n",
    "        ),\n",
    "        torch.nn.Sigmoid()\n",
    "    )\n",
    "    model1.load_state_dict(torch.load(trained_weights +'TBCells.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# validating dataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# defining hook to access output from intermidiate layer\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# registerning the hook. different layer can be accessed by changing\n",
    "# fc[3] parameter. for details \n",
    "# https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/05/27/extracting-features.html\n",
    "model.fc[3].register_forward_hook(get_features('feats'))\n",
    "\n",
    "\n",
    "trained_model_PATH = trained_weights +'TBCells.pt'\n",
    "model.load_state_dict(torch.load(trained_model_PATH))\n",
    "model.eval()\n",
    "#print(model)\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "FEATS = [] # extracted feats will be saved here\n",
    "# placeholder for batch features\n",
    "features = {}\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, target, path in test_dataloader:\n",
    "        image, target = image.to(DEVICE), target.to(DEVICE)\n",
    "        outputs = model(image)\n",
    "        output = (torch.max(outputs, 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        target = target.data.cpu().numpy()\n",
    "        \n",
    "        # not_matched = [target.index(y) for x, y in zip(y_pred, target) if y != x]\n",
    "        not_matched = np.where(y_pred != target)[0]\n",
    "        if not os.path.exists(trained_weights + \"wrong_pred/\"):\n",
    "            os.makedirs(trained_weights + \"wrong_pred/\")\n",
    "            for element in not_matched:\n",
    "                shutil.copy(path[element], trained_weights + \"wrong_pred/\" + path[element].split(\"/\")[-1]) \n",
    "        y_true.extend(target) # Save Truth\n",
    "        FEATS.extend((features['feats'].cpu().numpy())) # Save feature\n",
    "       \n",
    "print(f' Feature extracted with shape: {np.array(FEATS).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = labels_map.values()   \n",
    " \n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "class_accuracy=np.mean(100*cf_matrix.diagonal()/cf_matrix.sum(1))\n",
    "print(f'average acuraccy of all classes: {class_accuracy}')\n",
    "\n",
    "# ploting \n",
    "font = {'family' : 'Arial',\n",
    "        'size'   : 20}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "fig, ax = plt.subplots( dpi = 150)\n",
    "sn.heatmap(cf_matrix, annot=True, cmap='Blues',fmt='g')\n",
    "ax.set_title(f'Confusion Matrix, average acuracy: {class_accuracy :.2f}', fontsize = 15)\n",
    "ax.xaxis.set_ticklabels(classes); ax.yaxis.set_ticklabels(classes);\n",
    "plt.xlabel('Predicted'); plt.ylabel('True');\n",
    "\n",
    "plt.savefig(trained_weights + 'cf_matrix.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cls_report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\n",
    "df = pd.DataFrame(cls_report)\n",
    "# df.drop(columns=['accuracy'])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (9, 5))\n",
    "\n",
    "sns.heatmap(df.iloc[:-1, :].T, annot=True)\n",
    "fig.tight_layout()\n",
    "plt.savefig(trained_weights + \"classification_report.png\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-SNE plot\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import cm\n",
    "\n",
    "# Create a two dimensional t-SNE projection of the embeddings\n",
    "tsne = TSNE(2, verbose=0)\n",
    "tsne_proj = tsne.fit_transform(FEATS)\n",
    "# Plot those points as a scatter plot and label them based on the pred labels\n",
    "\n",
    "cmap = cm.get_cmap('tab20')\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "for lab in range(len(labels_map)):\n",
    "    indices = np.array(y_pred)==lab\n",
    "    #ax.scatter(tsne_proj[indices,0],tsne_proj[indices,1], c=np.array(cmap(lab)).reshape(1,4), label = lab ,alpha=0.5)\n",
    "    ax.scatter(tsne_proj[indices,0],tsne_proj[indices,1], label = lab ,alpha=0.5)\n",
    "ax.legend(fontsize='large', markerscale=2)\n",
    "plt.savefig(trained_weights + \"T-SNE.png\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keyboard\n",
    "#import keyboard\n",
    "#keyboard.press_and_release('command+s')\n",
    "\n",
    "nb_name = \"test.ipynb\"\n",
    "nb_full_path = os.path.join(os.getcwd(), nb_name)\n",
    "\n",
    "import shutil\n",
    "shutil.copyfile(nb_full_path, trained_weights  + nb_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52935d2f357a1e9b1e82836401a8d9e7860cec2909db9f1bc85e95f9b8c17282"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
